{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transform import data_transforms\n",
    "from WasteDataset import WasteDataset\n",
    "from WasteDataloader import WasteDataLoader\n",
    "from utils.image import (\n",
    "    rotate_ccw_45,\n",
    "    rotate_ccw_90,\n",
    "    rotate_ccw_135,\n",
    "    rotate_180,\n",
    "    rotate_cw_45,\n",
    "    rotate_cw_90,\n",
    "    rotate_cw_135\n",
    ")\n",
    "\n",
    "data_paths = json.load(open('./data/data_paths2.json', 'r', encoding='utf8'))\n",
    "augment_funcs = [\n",
    "    rotate_ccw_45,\n",
    "    rotate_ccw_90,\n",
    "    rotate_ccw_135,\n",
    "    rotate_180,\n",
    "    rotate_cw_45,\n",
    "    rotate_cw_90,\n",
    "    rotate_cw_135\n",
    "]\n",
    "batch_size = 64\n",
    "datasets = {}\n",
    "for key in data_paths.keys():\n",
    "    datasets[key] = WasteDataset(data_paths[key],\n",
    "                                 data_transforms[key],\n",
    "                                 augment_functions=[])\n",
    "\n",
    "waste_dataloader = WasteDataLoader(datasets=datasets,\n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_dataloader.get_dataset('test')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ResNetBaseClassifier import ResNetBaseClassifier\n",
    "from models.DenseNetBaseClassifier import DenseNetBaseClassifier\n",
    "\n",
    "class PretrainedModel:\n",
    "    resnet = 'Resnet'\n",
    "    densenet = 'Densenet'\n",
    "\n",
    "pretrained_model = PretrainedModel.densenet\n",
    "\n",
    "if pretrained_model == PretrainedModel.resnet:\n",
    "    model = ResNetBaseClassifier(waste_dataloader.n_classes())\n",
    "elif pretrained_model == PretrainedModel.densenet:\n",
    "    model = DenseNetBaseClassifier(waste_dataloader.n_classes(), 0)\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model.load_state_dict(torch.load('./checkpoints/waste_weights_resnet_22112023_9.h5'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    # print(y_target)\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    # _, y_target_indices = y_target.max(dim=1)\n",
    "    # print(y_pred_indices)\n",
    "    # print(y_target_indices)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# device = 'cuda'\n",
    "# model = model.to(device)\n",
    "# model.load_state_dict(torch.load('weights_40epoches.h5'))\n",
    "optimizer = optim.Adam(model.get_fc_parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.8,\n",
    "                                                 patience=1)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "from typing import Any, Set\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def classification_report_to_df(y_true: list, \n",
    "                                y_pred: list,\n",
    "                                labels: list) -> pd.DataFrame:\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    labels.extend(['accuracy', 'micro avg', 'weighted avg'])\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    # report.insert(0, 'Label', labels)\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "def classification_report_to_csv(y_true: list, \n",
    "                                 y_pred: list, \n",
    "                                 csv_filepath: str='') -> None:\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(csv_filepath)\n",
    "    \n",
    "    \n",
    "def conf_matrix(y_true: list, \n",
    "                y_pred: list,\n",
    "                labels: list) -> list:\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "from typing import Any, Set\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from mlflow_extend import mlflow as mlex\n",
    "from WasteDataset import WasteDataset\n",
    "\n",
    "\n",
    "def test(model: Any, \n",
    "         dataloader: DataLoader, \n",
    "         device: str='cuda') -> Set[list]:\n",
    "    \"\"\"\"\"\"\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        outputs = model(inputs.to(device))\n",
    "        pred_probs_of_batch = F.softmax(outputs, dim=1).cpu().data.numpy()\n",
    "        pred_labels_of_batch = np.argmax(pred_probs_of_batch, axis=1)\n",
    "        \n",
    "        pred_labels.extend(pred_labels_of_batch)\n",
    "        true_labels.extend(labels)\n",
    "    \n",
    "    return true_labels, pred_labels\n",
    "\n",
    "\n",
    "def log_metric(name, value, step):\n",
    "    \"\"\"Log a scalar value to both MLflow and TensorBoard\"\"\"\n",
    "    mlflow.log_metric(name, value, step=step)\n",
    "    \n",
    "    \n",
    "def log_param(key, value):\n",
    "    \"\"\"Log a scalar value to both MLflow and TensorBoard\"\"\"\n",
    "    mlflow.log_param(key, value)\n",
    "    \n",
    "    \n",
    "def log_model(model: Any, \n",
    "              model_name: str, \n",
    "              dataloader: DataLoader, \n",
    "              dataset: WasteDataset) -> None:\n",
    "    \"\"\"\"\"\"\n",
    "    mlflow.pytorch.log_model(model, model_name)  \n",
    "    \n",
    "    y_true, y_pred = test(model, dataloader)\n",
    "    df = classification_report_to_df(y_true, y_pred, list(dataset.get_non_empty_classes()))\n",
    "    \n",
    "    try:\n",
    "        cm = conf_matrix(y_true, y_pred, list(dataset.get_non_empty_classes(True)))\n",
    "        mlex.log_confusion_matrix(cm, f'{model_name}/confusion_matrix.png' )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    mlex.log_df(df, f'{model_name}/classification_report.csv')\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                data_loaders: WasteDataLoader,\n",
    "                loss_function: nn.CrossEntropyLoss,\n",
    "                optimizer: optim.Adam,\n",
    "                scheduler: optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                num_epochs: int=1,\n",
    "                device: str='cuda',\n",
    "                losses: dict={},\n",
    "                accuracies: dict={},\n",
    "                step_model_logging: int=5) -> nn.Module:\n",
    "    \"\"\"\"\"\"\n",
    "    phases = ['train', 'validation']\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in phases:\n",
    "            print('Phase:', phase)\n",
    "            if phase == 'train':\n",
    "                # scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(data_loaders.get_dataloader(phase)):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step(loss)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / data_loaders.size_of_set(phase)\n",
    "            epoch_acc = running_corrects.float() / data_loaders.size_of_set(phase)\n",
    "\n",
    "            losses[phase].append(epoch_loss.item())\n",
    "            accuracies[phase].append(epoch_acc.item())\n",
    "\n",
    "            log_metric(f'{phase}_loss', epoch_loss, epoch)\n",
    "            log_metric(f'{phase}_accuracy', float(epoch_acc), epoch)\n",
    "            \n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss.item(),\n",
    "                                                        epoch_acc.item()))\n",
    "\n",
    "        if (epoch + 1) % step_model_logging == 0:\n",
    "            log_model(model, \n",
    "                      f'model_{epoch + 1}', \n",
    "                      data_loaders.get_dataloader('test'),\n",
    "                      data_loaders.get_dataset('test'))\n",
    "        \n",
    "    return model, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = ['train', 'validation']\n",
    "losses = {}\n",
    "accuracies = {}\n",
    "for phase in phases:\n",
    "    losses[phase] = []\n",
    "    accuracies[phase] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pytorch\n",
    "from mlflow_extend import mlflow as mlex\n",
    "from datetime import date\n",
    "from time import time\n",
    "\n",
    "# today = str(date.today())\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "new_experiment = pretrained_model\n",
    "# mlflow.create_experiment(new_experiment)\n",
    "mlflow.set_experiment(new_experiment)\n",
    "\n",
    "n_epoches = 60\n",
    "mlflow.start_run(run_name=f'{pretrained_model}_test_2')\n",
    "mlflow.log_param('Number of classes', waste_dataloader.n_classes())\n",
    "mlflow.log_param('Batch size', batch_size)\n",
    "mlflow.log_param('Dataset', './images/')\n",
    "mlflow.log_param('Epoch', n_epoches)\n",
    "scripted_model = torch.jit.script(model)\n",
    "start = time()\n",
    "scripted_model, losses, accuracies = train_model(model=model,\n",
    "                                                 data_loaders=waste_dataloader,\n",
    "                                                 loss_function=loss_func,\n",
    "                                                 optimizer=optimizer,\n",
    "                                                 scheduler=scheduler,\n",
    "                                                 num_epochs=n_epoches,\n",
    "                                                 losses=losses,\n",
    "                                                 accuracies=accuracies)\n",
    "\n",
    "# model_name = 'model'\n",
    "# mlflow.pytorch.log_model(scripted_model, model_name)  # logging scripted model\n",
    "# mlflow.pytorch.save_model(scripted_model, today + '/')\n",
    "# model_path = mlflow.get_artifact_uri(\"model\")\n",
    "mlflow.log_param('Time', (time() - start) / 60)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './checkpoints/waste_weights_resnet_23112023_data2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./checkpoints/waste_weights_resnet_23112023_data2_l.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(losses, f)\n",
    "\n",
    "\n",
    "with open('./checkpoints/waste_weights_resnet_23112023_data2_a.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(accuracies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetBaseClassifier(waste_dataloader.n_classes() + 2)\n",
    "model.to('cuda')\n",
    "model.load_state_dict(torch.load('./checkpoints/waste_weights_resnet_22112023_9.h5'))\n",
    "model.eval()\n",
    "# model_trained = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_dataloader.get_dataloader('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "device = 'cuda'\n",
    "\n",
    "labels = waste_dataloader.get_classes().copy()\n",
    "# labels\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "# for sample in datasets['test']:\n",
    "test_paths = datasets['test'].img_paths\n",
    "test_labels = datasets['test'].img_labels\n",
    "pred_labels = []\n",
    "for path in tqdm(test_paths):\n",
    "    img = Image.open(path)\n",
    "\n",
    "    validation_batch = data_transforms['test'](img).to(device)\n",
    "    validation_batch = torch.stack([validation_batch])\n",
    "    pred_logits_tensor = model_trained(torch.Tensor(validation_batch))\n",
    "    pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n",
    "    pred_label = np.argmax(pred_probs[0])\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "    # if pred_label != test_labels[i]:\n",
    "    #     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.remove('37.Nhiệt kế thủy ngân')\n",
    "labels.remove('38.Sơn, Vecni, Dung môi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# labels.remove('37.Nhiệt kế thủy ngân')\n",
    "# labels.remove('38.Sơn, Vecni, Dung môi')\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = list(range(0, waste_dataloader.n_classes())))\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_dataloader.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = waste_dataloader.get_classes()\n",
    "validation_img_paths = [\n",
    "            \"./images/0650.png\",\n",
    "            \"./images/0697.png\",\n",
    "            \"./images/0704.png\",\n",
    "            \"./images/0655.png\",\n",
    "            \"./images/0675.png\",\n",
    "            \"./images/0681.png\",\n",
    "]\n",
    "from PIL import Image\n",
    "img_list = [Image.open(img_path) for img_path in validation_img_paths]\n",
    "\n",
    "validation_batch = torch.stack([data_transforms['validation'](img).to(device)\n",
    "                                for img in img_list])\n",
    "\n",
    "pred_logits_tensor = model_trained(validation_batch)\n",
    "pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, len(img_list), figsize=(20, 5))\n",
    "for i, img in enumerate(img_list):\n",
    "    ax = axs[i]\n",
    "    ax.axis('off')\n",
    "    # print(np.argmax(pred_probs[i]))\n",
    "    ax.set_title(labels[np.argmax(pred_probs[i])])\n",
    "    ax.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
